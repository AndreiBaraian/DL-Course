{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 521153S, Deep Learning assignment 2: Backpropagation, gradient check using linear approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "#### In this assignment, you will learn:\n",
    "* **Backpropagation**.\n",
    "* Building a one layer NN using numpy.\n",
    "* Gradient check using finite-difference approximation.\n",
    "* Stochatic Gradient Descent (SGD).\n",
    "* Simple hype-parameters tuning methods to improve your NN performance.\n",
    "\n",
    "#### Grading (<span style=\"color:green\">15 points</span>)\n",
    "In this assignment, we are going to learn about **backpropagation**. This is a very importance algorithm you need to know when building and training your neural network.\n",
    "* **Part 1.** Import libraries, loading and preprocessing the training and testing data.\n",
    "* **Part 2.** Backpropagation. (<span style=\"color:green\">10 points</span>)\n",
    "  * 2.1. Construct model in **Fig. 2** (<span style=\"color:green\">2 points</span>) <br>\n",
    "       * 2.1.1 Declare W1 and W2 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.1.2 Implement sigmoid (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.1.3 Implement softmax (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.1.4 Implement the cross-entropy loss (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "  * 2.2. Implement the forward-pass (<span style=\"color:green\">1.5 point</span>) <br>\n",
    "       * 2.2.1 Foward-pass in training (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.2.2 Call cross entropy loss in training (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.2.3 Forward-pass in testing (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "  * 2.3. Implement SGD (<span style=\"color:green\">1 point</span>) <br>\n",
    "       * 2.3.1 Velocity (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.3.2 Update weights (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "  * 2.4. Derive the derivatives (<span style=\"color:green\">3.0 points</span>) <br>\n",
    "       * Derive equation 6->12, which Eq. 6+7 (<span style=\"color:green\">0.5 point</span>), and from Eq. 8-12 (<span style=\"color:green\">0.5 point</span>) each <br>\n",
    "  * 2.5. Implement the derivatives (<span style=\"color:green\">2.5 points</span>) <br>\n",
    "       * 2.5.1 Implement equation 8 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.5.2 Implement equation 6 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.5.3 Implement equation 10 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.5.4 Implement equation 10+11 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 2.5.5 Implement equation 7 (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "* **Part 3.** Gradient check using finite-difference approximation. (<span style=\"color:green\">3 points</span>) <br>\n",
    "  * 3.1. Implement gradient check for the model in **Fig. 2** (<span style=\"color:green\">2.5 points</span>) <br>\n",
    "       * 3.1.1 Implement reshape_params function (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 3.1.2 Adding the epsilon to params then reshape and split flatten_params to hidden weights (W1) and output weights (W2) (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 3.1.3 Implement the forward-pass (<span style=\"color:green\">0.125 point</span>) <br>\n",
    "       * 3.1.4 Call cross entropy loss in gradient check (<span style=\"color:green\">0.125 point</span>) <br>\n",
    "       * 3.1.5 Substracting the epsilon and do the similar calculations as above (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "       * 3.1.6 Implement the forward-pass (<span style=\"color:green\">0.125 point</span>) <br>\n",
    "       * 3.1.7 Call cross entropy loss in gradient check (<span style=\"color:green\">0.125 point</span>) <br>\n",
    "       * 3.1.8 Compute the numerical gradient using the idea in the toy example (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "  * 3.2. Question: why don't we use FDA to calculate the gradient to update our model? (<span style=\"color:green\">0.5 point</span>) <br>\n",
    "* **Part 4.** Regularization and NN simple tunning. (<span style=\"color:green\">2 points</span>) <br>\n",
    "  * 4.1. Applying weight decay. (<span style=\"color:green\">1 point</span>) <br>\n",
    "       * There are 5 spots you need to fill, (<span style=\"color:green\">0.2 point</span>) each spot.\n",
    "  * 4.2. Change the number of neurons in the hidden layer and report the performance (<span style=\"color:green\">1 point</span>) <br>\n",
    "\n",
    "#### Environment\n",
    "Python 3, Numpy, matplotlib, sklearn\n",
    "\n",
    "#### Dataset\n",
    "* [**Fashion-MNIST**](https://github.com/zalandoresearch/fashion-mnist)\n",
    "is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. Using the Fashion-MNIST give you more room to wiggle your experiments.\n",
    "\n",
    "#### Hints\n",
    "* To find the place where you have to insert your solution, hit Crtl + F and search for **TODO:** . You are NOT suppose to modify the codes from other parts.\n",
    "* **Be careful with the shape** of the weights, gradient, .. of your tensor in your implementation. Double check and make sure the shapes are fit for computation, especially matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Import libraries, loading and preprocessing the training and testing data\n",
    "**You don't need to change the code from this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# You will mainly use numpy to construct your NN\n",
    "import numpy as np\n",
    "import matplotlib, time, copy, os, requests, zipfile, sys\n",
    "# Matplotlib to plot the image\n",
    "import matplotlib.pyplot as plt\n",
    "# Off-the-shelf evaluation functions provided by sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Matplotlib predefined 'magic function'. It will include your graphs in your notebook, next to the code\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions use to download the dataset from google drive\n",
    "The code snipet was taken from [this thread](https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions use to pre-process your training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reshape_train_data(X):\n",
    "    ''' Input training data has shape (60000, 28, 28)\n",
    "        Input testing data has shape (10000, 28, 28)\n",
    "        where: \n",
    "        60000 is the numbers of input training samples\n",
    "        10000 is the numbers of input testing samples\n",
    "        similar to MNIST, resolution of each sample is 28 x 28\n",
    "    '''\n",
    "    samples, H, W = X.shape\n",
    "    # Reshape input volume to (sample, 784), this mean, your NN input layer will have 784 placeholders\n",
    "    # we scale the RGB values by divide them by 255, this will help improve the training performance\n",
    "    return X.reshape(samples, H * W).T / 255\n",
    "\n",
    "def one_hot_vector(x, num_classes):\n",
    "    # By now, I think you already heard about this so many times\n",
    "    return np.eye(num_classes)[x].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We took care of download the data for you.\n",
    "The fashion-MNIST data will be download and store in your **work_dir/data/fashion_mnist_npy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was already downloaded and extracted!\n"
     ]
    }
   ],
   "source": [
    "PATH = './data'\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "    \n",
    "    file_id = '1DQ2Nf2rY467kyZKOf_CG3Kib5FLv0xQu'\n",
    "    destination = os.path.join(PATH, 'fashion_mnist_npy.zip')\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "    \n",
    "    with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "        zip_ref.extractall(PATH)\n",
    "        \n",
    "    print(\"Data downloaded and extracted!\")\n",
    "    \n",
    "    os.remove(destination)\n",
    "    \n",
    "else:\n",
    "    print(\"Data was already downloaded and extracted!\")\n",
    "\n",
    "PATH = os.path.join(PATH, 'fashion_mnist_npy')\n",
    "\n",
    "# The actual meaning of the label of your classes.\n",
    "# E.g. if a output one-hot vector is [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], it used to prepresent a Dress\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the training input\n",
    "X_train = np.load(os.path.join(PATH, 'train_data.npy'))\n",
    "# Load the training labels\n",
    "X_test = np.load(os.path.join(PATH, 'test_data.npy'))\n",
    "# Load the testing input\n",
    "Y_train = np.load(os.path.join(PATH, 'train_labels.npy'))\n",
    "# Load the testing labels\n",
    "Y_test = np.load(os.path.join(PATH, 'test_labels.npy'))\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(label_names)\n",
    "# Get the number of training samples and their resolution for reshape\n",
    "num_trains, HEIGHT, WIDTH = X_train.shape\n",
    "\n",
    "# Reshape the training and testing inputs\n",
    "X_train, X_test = reshape_train_data(X_train), reshape_train_data(X_test)\n",
    "\n",
    "# Create one-hot vector for the training and testing labels\n",
    "Y_train, Y_test = one_hot_vector(Y_train, num_classes), one_hot_vector(Y_test, num_classes)\n",
    "\n",
    "\n",
    "# This part use to randomly load some of the training and testing image and the one-hot vectors for checking\n",
    "fig_train, ax_train = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_train.suptitle(\"Random image from the TRAINING set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "fig_test, ax_test = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_test.suptitle(\"Random image from the TESTING set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx in range(5):\n",
    "    i, j = np.random.randint(num_trains), np.random.randint(X_test.shape[0])\n",
    "    \n",
    "    ax_train[idx].imshow(X_train[:,i].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
    "    ax_train[idx].set_title(label_names[np.argmax(Y_train[:,i])] + \"\\n\" + str(Y_train[:,i]))\n",
    "    ax_train[idx].axis('off')\n",
    "    \n",
    "    ax_test[idx].imshow(X_test[:,j].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
    "    ax_test[idx].set_title(label_names[np.argmax(Y_test[:,j])] + \"\\n\" + str(Y_test[:,j]))\n",
    "    ax_test[idx].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Backpropagation (<span style=\"color:green\">10 points</span>)\n",
    "#### The fun part start from here.\n",
    "*Definition:* backpropagation is an algorithm used to **calculate the gradient of a loss function** (sometime called cost function), by applying the chain rule, so that we can update the weights (and bias) of our neural network using optimizer such as gradient descent. <br>\n",
    "It could be interpreted as the backward propagation (oppose to the forward propagation) of the network’s layers error, in which we calculate the error of the current layer, then pass the weighted error back to the previous layer, and recursively doing this until we travel back to the first hidden layer. At each layer, we update the weights using the derivative of the cost for each weight.\n",
    "\n",
    "Let illustrate this concept using a simple toy example.\n",
    "\n",
    "![](img/toy.png)\n",
    "<center> <strong> <font size=\"3\" color=\"blue\"> Figure 1. Toy example of backpropagation </font> </strong> </center>\n",
    "\n",
    "In **Fig. 1** we have a loss function $L$:\n",
    "\\begin{align}\n",
    "L = c \\times d\n",
    "\\end{align}\n",
    "\n",
    "where <br>\n",
    "\n",
    "\\begin{align}\n",
    "c = a + b -5\n",
    "\\end{align}\n",
    "\n",
    "and, <br>\n",
    "\n",
    "\\begin{align}\n",
    "d = b^2 + b -1\n",
    "\\end{align}\n",
    "\n",
    "As illustrated in **Fig. 1**, the equations in the edge show the partial derivation of some functions with respect to their direct variables. For example, $\\frac{\\partial L}{\\partial c}$ is the partial derivative of $L$ with respect to $c$. <br>\n",
    "However, we are actually interested in calculate the derivative of $L$ with respect to $a$ and $b$, which don't directly connected with each other. So, how can we do this? As mentioned earlier, we can do this using the chain-rule so that we can calculate $\\frac{\\partial L}{\\partial a}$ and $\\frac{\\partial L}{\\partial b}$ as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a}\n",
    "\\end{align}\n",
    "\n",
    "and, <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} + \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial b}\n",
    "\\end{align}\n",
    "\n",
    "More importantly, **we can see that $a$ affect $L$ through $c$, and so on**. This concept hold no matter how many hidden layers you have or how complicated your loss will be as long as you use backpropagation to calculate the derivative to update your weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get your hand dirty. Let's take a look at the model in **Fig. 2** below. This is the model we are gonna use in this assignment.\n",
    "\n",
    "![](img/model.png)\n",
    "<center> <strong> <font size=\"4\" color=\"blue\"> Figure 2. Structure of our neural network </font> </strong> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model base on **Fig. 2** above (<span style=\"color:green\">2 points</span>)\n",
    "As shown in the **Fig. 2**, our neural network contains one hidden layer (we use 5 neurons as default setting) and an output layer. To further simply things, we will not include biases in our model. Be aware that, while the number of neuron in the hidden layer can be abitrary, there must be 10 neurons in the output layer because we want our model to classify images from 10 classes. Based on the **Fig. 2**, you have to construct your network. Don't worry, it's easy and you already learnt how to do this in assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Numbers of input units\n",
    "num_input = X_train.shape[0]\n",
    "# Number of neural in your hidden layer\n",
    "# TODO: modify the number of neurons in the hidden layer\n",
    "num_hidden = 5\n",
    "\n",
    "# Construct your neural network from Fig. 2 (0.5 point)\n",
    "# TODO: Random initialize the hidden layer weights (w/o bias)\n",
    "#W1 = np.random.normal(scale=1 / num_hidden**.5, size=num_input * num_hidden).reshape(num_hidden, num_input)\n",
    "W1 = np.random.randn(num_hidden,num_input) * np.sqrt(2 / num_hidden)#.reshape(num_hidden, num_input)\n",
    "# TODO: Random initialize the output layer weights (w/o bias)\n",
    "#W2 = np.random.normal(scale=1 / num_classes**.5, size=num_hidden * num_classes).reshape(num_classes, num_hidden)\n",
    "W2 = np.random.randn(num_classes,num_hidden) * np.sqrt(2 / num_hidden)\n",
    "#W1 = np.ones_like(W1)\n",
    "#W2 = np.ones_like(W2)\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "def softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    M = Y_pred.shape[1]\n",
    "    return -np.sum(np.multiply(Y,np.log(Y_pred)))/M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model in **Fig. 2**, our **forward-propagation** is going to start with the calculation of the output volume $A_1$ for the hidden layer:\n",
    "\n",
    "\\begin{align}\n",
    "A_1 = \\sigma(Z_1) = \\sigma(W_1 X ) \\tag{1}\n",
    "\\end{align}\n",
    "where $W_1$ is the weights of the hidden layer, $X$ is the input, and $\\sigma$ is the sigmoid activation where: <br>\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-1}} \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "Then, we calculate the output volume of the output layer $A_2$ by: <br>\n",
    "\\begin{align}\n",
    "A_2 = S(Z_2) = S(W_2 A_1) \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "In **Eq. 3**, $W_2$ is the weights of the output layer, $A_1$ is the output volume of the hidden layer, $S$ stands for softmax and defines by: <br>\n",
    "\\begin{align}\n",
    "S(x_i) = \\frac{e^{x_i}}{\\sum_{j=0}^{k} e^{x_j} } \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Finally, we compute the loss function using:\n",
    "\n",
    "\\begin{align}\n",
    "L(Y, A_2) = - \\frac{1}{M} \\sum_{k=0}^{M} \\sum_{i=0}^{N} Y_i^{k} log({A_2}_i^{k}) \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "where $Y$ is the ground truth labels, $N$ is the number of classes, $M$ is the number of samples in the training batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the derivative and fill the equation 6->12 below (<span style=\"color:green\">3.0 points</span>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you learnt in undergraduate school, let's calculate the **backward-propagation** for our model in **Fig. 2**.<br>\n",
    "As mentioned earlier, we are interested in $\\frac{\\partial L}{\\partial W_1}$ and $\\frac{\\partial L}{\\partial W_2}$, where: (<span style=\"color:green\">0.5 point</span>)\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial A_2} \\cdot \\frac{\\partial A_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial W_2} = (A_2-Y) \\cdot A_1 \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "and, <br> \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial A_1} \\cdot \\frac{\\partial A_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1} = (A_2 - Y) \\cdot W_2 \\cdot A_1 \\cdot (1 - A_1) \\cdot X \\tag{7}\n",
    "\\end{align}\n",
    " \n",
    "From **Eq. 6** we have: <br>\n",
    "\n",
    "The derivative of $L$ with respect to $Z_2$: (<span style=\"color:green\">0.5 point</span>)\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial A_2} \\cdot \\frac{\\partial A_2}{\\partial Z_2} = A_2 - Y \\tag{8}\n",
    "\\end{align}\n",
    "\n",
    "The derivative of $Z_2$ with respect to $W_2$: (<span style=\"color:green\">0.5 point</span>)\n",
    "\\begin{align}\n",
    "\\frac{\\partial Z_2}{\\partial W_2} = A_1 \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "From **Eq. 7** we have: <br>\n",
    "\n",
    "The derivative of $L$ with respect to $A_1$: (<span style=\"color:green\">0.5 point</span>)\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot W_2 = (A_2 - Y) \\cdot W_2 \\tag{10}\n",
    "\\end{align}\n",
    "\n",
    "The derivative of $A_1$ with respect to $Z_1$: (<span style=\"color:green\">0.5 point</span>)\n",
    "\\begin{align}\n",
    "\\frac{\\partial A_1}{\\partial Z_1} = A_1 \\cdot (1 - A_1) \\tag{11}\n",
    "\\end{align}\n",
    "\n",
    "The derivative of $Z_1$ with respect to $W_1$: (<span style=\"color:green\">0.5 point</span>)\n",
    "\\begin{align}\n",
    "\\frac{\\partial Z_1}{\\partial W_1} = X \\tag{12}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part. 3 Gradient check using finite-difference approximation. (<span style=\"color:green\">3 points</span>)\n",
    "\n",
    "When training deep neural network, there are many things that can go wrong. Until this point, you probably notice that we have been going on and on about the gradient. Why? Because the gradient is very important. Hence, you must making sure that the calculation of your gradient is correct. A correct gradient calculation won't promise your model will converge, but if the calculation was wrong your model will perform very weird. This type of error is hard to debug, so we better prevent it beforehand. <br> \n",
    "To perform the gradient check, we can calculate gradient using the [finite-difference approximation](https://en.wikipedia.org/wiki/Finite_difference) (FDA), and let's call the output of FDA numerical gradients. Then we compare this numerical gradients with the gradient we calculate from taking the derivative. If the differences between them are small enough, we can assume that the gradient was calculated correctly. <br>\n",
    "\n",
    "You probably learnt about FDA in your undergraduate, but to refresh your mind, let's have a simple example to see how FDA works. Assume that we have a function $f(x)$ which <br>\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{3} x^3 - \\frac{1}{2} x^2 + 1 \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "Then, the derivative $\\Delta f$ will be: <br>\n",
    "\\begin{align}\n",
    "\\Delta f = x^2 - x \\tag{14}\n",
    "\\end{align}\n",
    "\n",
    "At $x=2.125$, using **Eq. 14** we have $\\Delta f = 2.390625$\n",
    "\n",
    "If we calculate the numerical gradient using FDA we have:\n",
    "\\begin{align}\n",
    "\\Delta_{num\\_grad} f = \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2 * \\epsilon}  \\tag{15}\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is a very small value (E.g. $\\epsilon = 1e-{04}$)\n",
    "\n",
    "At the same point $x=2.125$, using **Eq. 15** we have $\\Delta_{num\\_grad} f = 2.3906250033389753$ <br>\n",
    "\n",
    "We can see that, the calculated values of $\\Delta f$ and $\\Delta_{num\\_grad} f$ are very close to each other. <br>\n",
    "\n",
    "With the same idea, we can check the gradient calculation of our network using FDA. A simple way to do this is: <br>\n",
    "1) We wiggle (by a very small $\\epsilon$ values) the value of our weight for all of the parameters in our model. By all parameters, I mean all of the weights of $W_1$ and $W_2$. E.g. if we use 5 neurons in the hidden layer, the number of parameters in our network is : <br> \n",
    "$num\\_params(net) = num\\_params(W_1) + num\\_params(W_2) = 28*28*5 + 5*10 = 3970$ <br>\n",
    "so we have to repeat the \"wiggling\" and calculate the numerical gradient 3970 times. At the end, we have a $num\\_grad$ vector that have shape (3970,) <br>\n",
    "2) Calculate the gradient by taking the derivative. Similarly, we will have a $grad$ vector that also have shape (3970,) <br>\n",
    "3) Compare $num\\_grad$ and $grad$ vectors by: <br>\n",
    "\\begin{align}\n",
    "grad\\_diff = \\frac{|grad - num\\_grad|_2}{|grad + num\\_grad|_2}  \\tag{16}\n",
    "\\end{align}\n",
    "\n",
    "If **grad_diff** is smaller than $1e-{08}$ than we assume that our gradient calculation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why don't we use FDA to calculate the gradient to update our model? (<span style=\"color:green\">0.5 points</span>) <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use FDA to calculate the gradient because it is VERY inefficient. It calculates the gradient for each weight in the whole network, leading to very big computations. It is much more efficient to calculate the gradient with back-propagation. Even for this example, if we use 5 hidden layers, running the algorithms with gradient check takes a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient check (<span style=\"color:green\">2.5 points</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reshape_params(params, num_input, num_hid, num_classes):\n",
    "    '''Reshape and split flatten_params to hidden weights (W1) and output weights (W2)\n",
    "        Input: \n",
    "            + params: the flatten parameters of W1 and W2\n",
    "            + num_input: the number of input units\n",
    "            + num_hid: the number of hidden neurons\n",
    "            + num_classes: the number of output class\n",
    "        Output:\n",
    "            + W1: weights of the hidden layer in shape (num_hid, num_input)\n",
    "            + W2: weights of the output layer in shape (num_class, num_hid)\n",
    "    ''' \n",
    "    \n",
    "    # TODO: implement this function (0.5 point)\n",
    "    W1 = params[0:num_input * num_hid]\n",
    "    W2 = params[num_input * num_hid:]\n",
    "    \n",
    "    W1 = W1.reshape(num_hid, num_input)\n",
    "    W2 = W2.reshape(num_classes, num_hid)\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "def calculate_numerical_gradient(_W1, _W2, X, y, is_weight_decay):\n",
    "    params_flatten = np.concatenate( (copy.deepcopy(_W1).ravel(), copy.deepcopy(_W2).ravel()) )\n",
    "    numgrad = np.zeros( len(params_flatten) )\n",
    "    wiggle = np.zeros( len(params_flatten) )\n",
    "    num_hidden, num_input = _W1.shape\n",
    "    num_classes = _W2.shape[0]\n",
    "    e = 1e-4\n",
    "\n",
    "    # Loop through each parameter in our model\n",
    "    for p in range(len(params_flatten)):\n",
    "        wiggle[p] = e\n",
    "        # TODO: adding the epsilon to params then reshape and split flatten_params to hidden weights (W1) and output weights (W2)\n",
    "        # (0.5 point)\n",
    "        params_flatten += wiggle\n",
    "        W1, W2 = reshape_params(params_flatten, num_input, num_hidden, num_classes)\n",
    "    \n",
    "    \n",
    "        # TODO: implement the forward-pass (0.125 point)\n",
    "        Z1 = np.matmul(W1,X)\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.matmul(W2,A1)\n",
    "        A2 = softmax(Z2)\n",
    "        \n",
    "        if is_weight_decay:\n",
    "            # TODO: call cross entropy loss with weight decay regularization\n",
    "            # (0.2 point)\n",
    "            L1 = cross_entropy_loss(Y, A2) + (lmda / (2 * X.shape[1])) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "            \n",
    "        else:\n",
    "            # TODO: call cross entropy loss (0.125 point)\n",
    "            L1 = cross_entropy_loss(Y, A2)\n",
    "\n",
    "            \n",
    "        # reset the params values\n",
    "        params_flatten -= wiggle\n",
    "        \n",
    "        \n",
    "        # TODO: substract the epsilon and do the similar calculations as above\n",
    "        # (0.5 point)\n",
    "        params_flatten -= wiggle\n",
    "        W1, W2 = reshape_params(params_flatten, num_input, num_hidden, num_classes)\n",
    "    \n",
    "        # TODO: implement the forward-pass (0.125 point)\n",
    "        Z1 = np.matmul(W1,X)\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.matmul(W2,A1)\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        if is_weight_decay:\n",
    "            # TODO: call cross entropy loss with weight decay regularization\n",
    "            # (0.2 point)\n",
    "            L2 = cross_entropy_loss(Y, A2) + (lmda / (2 * X.shape[1])) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "            \n",
    "        else:\n",
    "            # TODO: call cross entropy loss (0.125 point)\n",
    "            L2 = cross_entropy_loss(Y, A2)\n",
    "            \n",
    "            \n",
    "        # TODO: Compute the numerical gradient using the idea in the toy example (0.5 point)\n",
    "        numgrad[p] = (L1 - L2) / (2 * e)\n",
    "        \n",
    "        # reset the params values\n",
    "        params_flatten += wiggle\n",
    "\n",
    "        # Return the value we changed to zero:\n",
    "        wiggle[p] = 0\n",
    "\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters in your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_trains: 60000, num_batchs: 600\n"
     ]
    }
   ],
   "source": [
    "# Flag use to enable/disable gradient check and weight decay regularization\n",
    "is_gradient_check = False\n",
    "is_weight_decay = True\n",
    "\n",
    "if is_weight_decay:\n",
    "    # Setting lambda coefficient for weight decay\n",
    "    lmda = np.exp(-7)\n",
    "\n",
    "# Seting learning rate and momentum for SGD\n",
    "learning_rate = 0.25\n",
    "beta = 0.4\n",
    "\n",
    "# Seting the number of training epochs\n",
    "epoch = 50\n",
    "# Choose your batch size\n",
    "batch_size = 100\n",
    "# Calculate the number of training iterations base on the number of training samples and your batch size\n",
    "num_batchs = num_trains // batch_size\n",
    "\n",
    "print(\"Num_trains: {}, num_batchs: {}\".format(num_trains, num_batchs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your network (<span style=\"color:green\">4.5 points</span>)\n",
    "\n",
    "In this assignment, we will train our model using mini-batch stochatic gradient descent with momentum. To know more about this optimization algorithm, please check out this great [video](https://www.youtube.com/watch?v=k8fTYJPd3_I) from Dr. Andrew Ng. <br>\n",
    "For this assignment, we will use the implementation from **Eq. 17** and **Eq. 18** <br>\n",
    "\n",
    "\\begin{align}\n",
    "v_{dW} = \\beta v_{dW} + (1 - \\beta) dW \\tag{17}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "W = W - \\alpha v_{dW} \\tag{18}\n",
    "\\end{align}\n",
    "\n",
    "If you are curious, you can modify the hyper-params in the above section at your will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch/Iterations]:[0/0], loss: 2.9264690084096148\n",
      "[Epoch/Iterations]:[0/100], loss: 0.8843794438836455\n",
      "[Epoch/Iterations]:[0/200], loss: 0.8017697477692205\n",
      "[Epoch/Iterations]:[0/300], loss: 0.7261629745529656\n",
      "[Epoch/Iterations]:[0/400], loss: 0.6082324303152847\n",
      "[Epoch/Iterations]:[0/500], loss: 0.7532192085193578\n",
      "=> Elapsed time epoch #0 : 0.66 seconds\n",
      "[Epoch/Iterations]:[1/0], loss: 0.7127333961864445\n",
      "[Epoch/Iterations]:[1/100], loss: 0.4855935567183343\n",
      "[Epoch/Iterations]:[1/200], loss: 0.4577546083028126\n",
      "[Epoch/Iterations]:[1/300], loss: 0.5826765219642226\n",
      "[Epoch/Iterations]:[1/400], loss: 0.4990164895984207\n",
      "[Epoch/Iterations]:[1/500], loss: 0.5419287600712231\n",
      "=> Elapsed time epoch #1 : 0.50 seconds\n",
      "[Epoch/Iterations]:[2/0], loss: 0.4403748752595719\n",
      "[Epoch/Iterations]:[2/100], loss: 0.4064624225393234\n",
      "[Epoch/Iterations]:[2/200], loss: 0.3804887955555097\n",
      "[Epoch/Iterations]:[2/300], loss: 0.46707597925314853\n",
      "[Epoch/Iterations]:[2/400], loss: 0.40322443173130984\n",
      "[Epoch/Iterations]:[2/500], loss: 0.44284529310793314\n",
      "=> Elapsed time epoch #2 : 0.51 seconds\n",
      "[Epoch/Iterations]:[3/0], loss: 0.3772572127719737\n",
      "[Epoch/Iterations]:[3/100], loss: 0.36775552061808015\n",
      "[Epoch/Iterations]:[3/200], loss: 0.3712240860604564\n",
      "[Epoch/Iterations]:[3/300], loss: 0.5906186788255755\n",
      "[Epoch/Iterations]:[3/400], loss: 0.3978439916359047\n",
      "[Epoch/Iterations]:[3/500], loss: 0.3930195665510185\n",
      "=> Elapsed time epoch #3 : 0.49 seconds\n",
      "[Epoch/Iterations]:[4/0], loss: 0.4638446998346678\n",
      "[Epoch/Iterations]:[4/100], loss: 0.38216211395582783\n",
      "[Epoch/Iterations]:[4/200], loss: 0.3106627912983313\n",
      "[Epoch/Iterations]:[4/300], loss: 0.4762864092162187\n",
      "[Epoch/Iterations]:[4/400], loss: 0.5732742350719787\n",
      "[Epoch/Iterations]:[4/500], loss: 0.4137668141805937\n",
      "=> Elapsed time epoch #4 : 0.50 seconds\n",
      "[Epoch/Iterations]:[5/0], loss: 0.3618946455616652\n",
      "[Epoch/Iterations]:[5/100], loss: 0.4412832746339517\n",
      "[Epoch/Iterations]:[5/200], loss: 0.3302390501472701\n",
      "[Epoch/Iterations]:[5/300], loss: 0.39199937775577703\n",
      "[Epoch/Iterations]:[5/400], loss: 0.4441220060712598\n",
      "[Epoch/Iterations]:[5/500], loss: 0.3707049654586169\n",
      "=> Elapsed time epoch #5 : 0.52 seconds\n",
      "[Epoch/Iterations]:[6/0], loss: 0.36172726395420607\n",
      "[Epoch/Iterations]:[6/100], loss: 0.4231577991671034\n",
      "[Epoch/Iterations]:[6/200], loss: 0.46425791973422464\n",
      "[Epoch/Iterations]:[6/300], loss: 0.43701119517944825\n",
      "[Epoch/Iterations]:[6/400], loss: 0.45640924477564176\n",
      "[Epoch/Iterations]:[6/500], loss: 0.2927506167899631\n",
      "=> Elapsed time epoch #6 : 0.70 seconds\n",
      "[Epoch/Iterations]:[7/0], loss: 0.4168998696869857\n",
      "[Epoch/Iterations]:[7/100], loss: 0.4516684114898726\n",
      "[Epoch/Iterations]:[7/200], loss: 0.3772468004533298\n",
      "[Epoch/Iterations]:[7/300], loss: 0.3209310376329672\n",
      "[Epoch/Iterations]:[7/400], loss: 0.4048875996797592\n",
      "[Epoch/Iterations]:[7/500], loss: 0.3443019834899264\n",
      "=> Elapsed time epoch #7 : 0.50 seconds\n",
      "[Epoch/Iterations]:[8/0], loss: 0.5331511623553181\n",
      "[Epoch/Iterations]:[8/100], loss: 0.424636392823274\n",
      "[Epoch/Iterations]:[8/200], loss: 0.45172056804156774\n",
      "[Epoch/Iterations]:[8/300], loss: 0.4008465373038461\n",
      "[Epoch/Iterations]:[8/400], loss: 0.3389438005358069\n",
      "[Epoch/Iterations]:[8/500], loss: 0.2741648587746859\n",
      "=> Elapsed time epoch #8 : 0.59 seconds\n",
      "[Epoch/Iterations]:[9/0], loss: 0.4103911923138948\n",
      "[Epoch/Iterations]:[9/100], loss: 0.2938009646703043\n",
      "[Epoch/Iterations]:[9/200], loss: 0.2951441657517422\n",
      "[Epoch/Iterations]:[9/300], loss: 0.2963330516871676\n",
      "[Epoch/Iterations]:[9/400], loss: 0.5309612318863698\n",
      "[Epoch/Iterations]:[9/500], loss: 0.4818840613036279\n",
      "=> Elapsed time epoch #9 : 0.55 seconds\n",
      "[Epoch/Iterations]:[10/0], loss: 0.38777078965236633\n",
      "[Epoch/Iterations]:[10/100], loss: 0.3599715704699461\n",
      "[Epoch/Iterations]:[10/200], loss: 0.29320162821638945\n",
      "[Epoch/Iterations]:[10/300], loss: 0.4055920257644958\n",
      "[Epoch/Iterations]:[10/400], loss: 0.38568733353249224\n",
      "[Epoch/Iterations]:[10/500], loss: 0.36271538704632084\n",
      "=> Elapsed time epoch #10 : 0.60 seconds\n",
      "[Epoch/Iterations]:[11/0], loss: 0.4952529266123026\n",
      "[Epoch/Iterations]:[11/100], loss: 0.3755242087321227\n",
      "[Epoch/Iterations]:[11/200], loss: 0.33960104980247735\n",
      "[Epoch/Iterations]:[11/300], loss: 0.2417300357144201\n",
      "[Epoch/Iterations]:[11/400], loss: 0.31312162208590505\n",
      "[Epoch/Iterations]:[11/500], loss: 0.2948656503840786\n",
      "=> Elapsed time epoch #11 : 0.53 seconds\n",
      "[Epoch/Iterations]:[12/0], loss: 0.4165868696233997\n",
      "[Epoch/Iterations]:[12/100], loss: 0.2948411460034088\n",
      "[Epoch/Iterations]:[12/200], loss: 0.45450840388430225\n",
      "[Epoch/Iterations]:[12/300], loss: 0.33502970277999494\n",
      "[Epoch/Iterations]:[12/400], loss: 0.25222532642920287\n",
      "[Epoch/Iterations]:[12/500], loss: 0.2893902253976393\n",
      "=> Elapsed time epoch #12 : 0.73 seconds\n",
      "[Epoch/Iterations]:[13/0], loss: 0.3911021472587587\n",
      "[Epoch/Iterations]:[13/100], loss: 0.3816126994467411\n",
      "[Epoch/Iterations]:[13/200], loss: 0.3571580877802003\n",
      "[Epoch/Iterations]:[13/300], loss: 0.2802615673480727\n",
      "[Epoch/Iterations]:[13/400], loss: 0.32787623496061796\n",
      "[Epoch/Iterations]:[13/500], loss: 0.1911195367333559\n",
      "=> Elapsed time epoch #13 : 0.48 seconds\n",
      "[Epoch/Iterations]:[14/0], loss: 0.3358281500312502\n",
      "[Epoch/Iterations]:[14/100], loss: 0.3135361460875894\n",
      "[Epoch/Iterations]:[14/200], loss: 0.31409118546096054\n",
      "[Epoch/Iterations]:[14/300], loss: 0.33976249600634356\n",
      "[Epoch/Iterations]:[14/400], loss: 0.35539026660571943\n",
      "[Epoch/Iterations]:[14/500], loss: 0.34843044194385786\n",
      "=> Elapsed time epoch #14 : 0.49 seconds\n",
      "[Epoch/Iterations]:[15/0], loss: 0.3167140720962387\n",
      "[Epoch/Iterations]:[15/100], loss: 0.311976113337576\n",
      "[Epoch/Iterations]:[15/200], loss: 0.39081683215740287\n",
      "[Epoch/Iterations]:[15/300], loss: 0.3479526431154936\n",
      "[Epoch/Iterations]:[15/400], loss: 0.29630461865032315\n",
      "[Epoch/Iterations]:[15/500], loss: 0.3980657226660803\n",
      "=> Elapsed time epoch #15 : 0.49 seconds\n",
      "[Epoch/Iterations]:[16/0], loss: 0.38194697553540147\n",
      "[Epoch/Iterations]:[16/100], loss: 0.2757052700326981\n",
      "[Epoch/Iterations]:[16/200], loss: 0.3898887003964045\n",
      "[Epoch/Iterations]:[16/300], loss: 0.3116116034237778\n",
      "[Epoch/Iterations]:[16/400], loss: 0.2882734772607691\n",
      "[Epoch/Iterations]:[16/500], loss: 0.29754498799074086\n",
      "=> Elapsed time epoch #16 : 0.49 seconds\n",
      "[Epoch/Iterations]:[17/0], loss: 0.39994944583954284\n",
      "[Epoch/Iterations]:[17/100], loss: 0.30813749622617287\n",
      "[Epoch/Iterations]:[17/200], loss: 0.433574186841761\n",
      "[Epoch/Iterations]:[17/300], loss: 0.39624790625119677\n",
      "[Epoch/Iterations]:[17/400], loss: 0.2645924472823264\n",
      "[Epoch/Iterations]:[17/500], loss: 0.33926383538626087\n",
      "=> Elapsed time epoch #17 : 0.55 seconds\n",
      "[Epoch/Iterations]:[18/0], loss: 0.27879394195710705\n",
      "[Epoch/Iterations]:[18/100], loss: 0.23280630772338595\n",
      "[Epoch/Iterations]:[18/200], loss: 0.277319701297385\n",
      "[Epoch/Iterations]:[18/300], loss: 0.24071881272662327\n",
      "[Epoch/Iterations]:[18/400], loss: 0.25110376011189683\n",
      "[Epoch/Iterations]:[18/500], loss: 0.2857806383516821\n",
      "=> Elapsed time epoch #18 : 0.52 seconds\n",
      "[Epoch/Iterations]:[19/0], loss: 0.25662151013832324\n",
      "[Epoch/Iterations]:[19/100], loss: 0.2716147966715839\n",
      "[Epoch/Iterations]:[19/200], loss: 0.2841828086484628\n",
      "[Epoch/Iterations]:[19/300], loss: 0.3293427757960788\n",
      "[Epoch/Iterations]:[19/400], loss: 0.4077951761295714\n",
      "[Epoch/Iterations]:[19/500], loss: 0.33968856722791557\n",
      "=> Elapsed time epoch #19 : 0.51 seconds\n",
      "[Epoch/Iterations]:[20/0], loss: 0.2483229680144397\n",
      "[Epoch/Iterations]:[20/100], loss: 0.28052863660693766\n",
      "[Epoch/Iterations]:[20/200], loss: 0.3848530097112867\n",
      "[Epoch/Iterations]:[20/300], loss: 0.25636398680568007\n",
      "[Epoch/Iterations]:[20/400], loss: 0.30796543453433267\n",
      "[Epoch/Iterations]:[20/500], loss: 0.2952032111296009\n",
      "=> Elapsed time epoch #20 : 0.50 seconds\n",
      "[Epoch/Iterations]:[21/0], loss: 0.21310491866279602\n",
      "[Epoch/Iterations]:[21/100], loss: 0.2685652183843682\n",
      "[Epoch/Iterations]:[21/200], loss: 0.3334087826213351\n",
      "[Epoch/Iterations]:[21/300], loss: 0.2692960991887722\n",
      "[Epoch/Iterations]:[21/400], loss: 0.259744737434655\n",
      "[Epoch/Iterations]:[21/500], loss: 0.3900585931072848\n",
      "=> Elapsed time epoch #21 : 0.52 seconds\n",
      "[Epoch/Iterations]:[22/0], loss: 0.2288764503820236\n",
      "[Epoch/Iterations]:[22/100], loss: 0.1746625325398508\n",
      "[Epoch/Iterations]:[22/200], loss: 0.24332038800983247\n",
      "[Epoch/Iterations]:[22/300], loss: 0.26127712408063686\n",
      "[Epoch/Iterations]:[22/400], loss: 0.32137373276670195\n",
      "[Epoch/Iterations]:[22/500], loss: 0.2617853354504127\n",
      "=> Elapsed time epoch #22 : 0.50 seconds\n",
      "[Epoch/Iterations]:[23/0], loss: 0.32734729312074234\n",
      "[Epoch/Iterations]:[23/100], loss: 0.32231183025497384\n",
      "[Epoch/Iterations]:[23/200], loss: 0.2177211334328182\n",
      "[Epoch/Iterations]:[23/300], loss: 0.3522766290861116\n",
      "[Epoch/Iterations]:[23/400], loss: 0.4753380092868251\n",
      "[Epoch/Iterations]:[23/500], loss: 0.415602781598461\n",
      "=> Elapsed time epoch #23 : 0.49 seconds\n",
      "[Epoch/Iterations]:[24/0], loss: 0.37215676434767153\n",
      "[Epoch/Iterations]:[24/100], loss: 0.3385646816364701\n",
      "[Epoch/Iterations]:[24/200], loss: 0.282484523455674\n",
      "[Epoch/Iterations]:[24/300], loss: 0.2948483808363863\n",
      "[Epoch/Iterations]:[24/400], loss: 0.21683286776122634\n",
      "[Epoch/Iterations]:[24/500], loss: 0.24004984346559244\n",
      "=> Elapsed time epoch #24 : 0.49 seconds\n",
      "[Epoch/Iterations]:[25/0], loss: 0.24583756575708904\n",
      "[Epoch/Iterations]:[25/100], loss: 0.4442144616468211\n",
      "[Epoch/Iterations]:[25/200], loss: 0.4687729349477103\n",
      "[Epoch/Iterations]:[25/300], loss: 0.3716397420652474\n",
      "[Epoch/Iterations]:[25/400], loss: 0.3813134759480374\n",
      "[Epoch/Iterations]:[25/500], loss: 0.12462795719414463\n",
      "=> Elapsed time epoch #25 : 0.50 seconds\n",
      "[Epoch/Iterations]:[26/0], loss: 0.2699800640168746\n",
      "[Epoch/Iterations]:[26/100], loss: 0.3019806409425565\n",
      "[Epoch/Iterations]:[26/200], loss: 0.4041385115734616\n",
      "[Epoch/Iterations]:[26/300], loss: 0.29709935875399196\n",
      "[Epoch/Iterations]:[26/400], loss: 0.23447505272531366\n",
      "[Epoch/Iterations]:[26/500], loss: 0.2518722061489057\n",
      "=> Elapsed time epoch #26 : 0.49 seconds\n",
      "[Epoch/Iterations]:[27/0], loss: 0.35254838980544984\n",
      "[Epoch/Iterations]:[27/100], loss: 0.25638488561574757\n",
      "[Epoch/Iterations]:[27/200], loss: 0.3978355375812876\n",
      "[Epoch/Iterations]:[27/300], loss: 0.2743096373759504\n",
      "[Epoch/Iterations]:[27/400], loss: 0.42111904524655985\n",
      "[Epoch/Iterations]:[27/500], loss: 0.24994417021156393\n",
      "=> Elapsed time epoch #27 : 0.49 seconds\n",
      "[Epoch/Iterations]:[28/0], loss: 0.23982690176750388\n",
      "[Epoch/Iterations]:[28/100], loss: 0.24297434935298298\n",
      "[Epoch/Iterations]:[28/200], loss: 0.37765387728361594\n",
      "[Epoch/Iterations]:[28/300], loss: 0.19801695767780964\n",
      "[Epoch/Iterations]:[28/400], loss: 0.28012893640249453\n",
      "[Epoch/Iterations]:[28/500], loss: 0.3004508710423276\n",
      "=> Elapsed time epoch #28 : 0.49 seconds\n",
      "[Epoch/Iterations]:[29/0], loss: 0.2799640498271066\n",
      "[Epoch/Iterations]:[29/100], loss: 0.24117637014226093\n",
      "[Epoch/Iterations]:[29/200], loss: 0.2654883399161188\n",
      "[Epoch/Iterations]:[29/300], loss: 0.23892569422783316\n",
      "[Epoch/Iterations]:[29/400], loss: 0.30672608833621645\n",
      "[Epoch/Iterations]:[29/500], loss: 0.39149045279358824\n",
      "=> Elapsed time epoch #29 : 0.53 seconds\n",
      "[Epoch/Iterations]:[30/0], loss: 0.31264731990814987\n",
      "[Epoch/Iterations]:[30/100], loss: 0.3019533930057217\n",
      "[Epoch/Iterations]:[30/200], loss: 0.38718413080067593\n",
      "[Epoch/Iterations]:[30/300], loss: 0.3054229446657316\n",
      "[Epoch/Iterations]:[30/400], loss: 0.2342185972164858\n",
      "[Epoch/Iterations]:[30/500], loss: 0.32025366404506145\n",
      "=> Elapsed time epoch #30 : 0.50 seconds\n",
      "[Epoch/Iterations]:[31/0], loss: 0.23282722611213902\n",
      "[Epoch/Iterations]:[31/100], loss: 0.2887269659229236\n",
      "[Epoch/Iterations]:[31/200], loss: 0.2841756571306097\n",
      "[Epoch/Iterations]:[31/300], loss: 0.3348477755850463\n",
      "[Epoch/Iterations]:[31/400], loss: 0.31189865902097325\n",
      "[Epoch/Iterations]:[31/500], loss: 0.21644729433750198\n",
      "=> Elapsed time epoch #31 : 0.49 seconds\n",
      "[Epoch/Iterations]:[32/0], loss: 0.18897639802295513\n",
      "[Epoch/Iterations]:[32/100], loss: 0.2795640483384249\n",
      "[Epoch/Iterations]:[32/200], loss: 0.2741416539417201\n",
      "[Epoch/Iterations]:[32/300], loss: 0.34606670550992225\n",
      "[Epoch/Iterations]:[32/400], loss: 0.3689296636342806\n",
      "[Epoch/Iterations]:[32/500], loss: 0.1871877026502284\n",
      "=> Elapsed time epoch #32 : 0.51 seconds\n",
      "[Epoch/Iterations]:[33/0], loss: 0.28534595729406304\n",
      "[Epoch/Iterations]:[33/100], loss: 0.31105086560316536\n",
      "[Epoch/Iterations]:[33/200], loss: 0.28892719373222475\n",
      "[Epoch/Iterations]:[33/300], loss: 0.2639713026487724\n",
      "[Epoch/Iterations]:[33/400], loss: 0.3195104918587019\n",
      "[Epoch/Iterations]:[33/500], loss: 0.1954649130947005\n",
      "=> Elapsed time epoch #33 : 0.50 seconds\n",
      "[Epoch/Iterations]:[34/0], loss: 0.32767671212845895\n",
      "[Epoch/Iterations]:[34/100], loss: 0.2809786897555205\n",
      "[Epoch/Iterations]:[34/200], loss: 0.23822303250693017\n",
      "[Epoch/Iterations]:[34/300], loss: 0.2983802472323764\n",
      "[Epoch/Iterations]:[34/400], loss: 0.38707856275571695\n",
      "[Epoch/Iterations]:[34/500], loss: 0.30151258010332926\n",
      "=> Elapsed time epoch #34 : 0.49 seconds\n",
      "[Epoch/Iterations]:[35/0], loss: 0.3010444641918246\n",
      "[Epoch/Iterations]:[35/100], loss: 0.2624213437820407\n",
      "[Epoch/Iterations]:[35/200], loss: 0.2916334469523819\n",
      "[Epoch/Iterations]:[35/300], loss: 0.40394910267690515\n",
      "[Epoch/Iterations]:[35/400], loss: 0.39854866965924773\n",
      "[Epoch/Iterations]:[35/500], loss: 0.3561460632047768\n",
      "=> Elapsed time epoch #35 : 0.49 seconds\n",
      "[Epoch/Iterations]:[36/0], loss: 0.18465628888428545\n",
      "[Epoch/Iterations]:[36/100], loss: 0.21113128141954546\n",
      "[Epoch/Iterations]:[36/200], loss: 0.29401877955663197\n",
      "[Epoch/Iterations]:[36/300], loss: 0.2785807331807822\n",
      "[Epoch/Iterations]:[36/400], loss: 0.45704764965662253\n",
      "[Epoch/Iterations]:[36/500], loss: 0.16153927017838482\n",
      "=> Elapsed time epoch #36 : 0.49 seconds\n",
      "[Epoch/Iterations]:[37/0], loss: 0.24424003619132093\n",
      "[Epoch/Iterations]:[37/100], loss: 0.3567823798445011\n",
      "[Epoch/Iterations]:[37/200], loss: 0.29469975253636604\n",
      "[Epoch/Iterations]:[37/300], loss: 0.3280918412446106\n",
      "[Epoch/Iterations]:[37/400], loss: 0.2854105677806473\n",
      "[Epoch/Iterations]:[37/500], loss: 0.23240322407613356\n",
      "=> Elapsed time epoch #37 : 0.49 seconds\n",
      "[Epoch/Iterations]:[38/0], loss: 0.3118285813273682\n",
      "[Epoch/Iterations]:[38/100], loss: 0.41097329672232624\n",
      "[Epoch/Iterations]:[38/200], loss: 0.31355582218798056\n",
      "[Epoch/Iterations]:[38/300], loss: 0.36264153926233683\n",
      "[Epoch/Iterations]:[38/400], loss: 0.360446578509734\n",
      "[Epoch/Iterations]:[38/500], loss: 0.3127856758020316\n",
      "=> Elapsed time epoch #38 : 0.49 seconds\n",
      "[Epoch/Iterations]:[39/0], loss: 0.2440224748062077\n",
      "[Epoch/Iterations]:[39/100], loss: 0.2858517932564228\n",
      "[Epoch/Iterations]:[39/200], loss: 0.2033283232337247\n",
      "[Epoch/Iterations]:[39/300], loss: 0.26277095578621135\n",
      "[Epoch/Iterations]:[39/400], loss: 0.2746319043007992\n",
      "[Epoch/Iterations]:[39/500], loss: 0.18083933517433462\n",
      "=> Elapsed time epoch #39 : 0.49 seconds\n",
      "[Epoch/Iterations]:[40/0], loss: 0.27950165023694884\n",
      "[Epoch/Iterations]:[40/100], loss: 0.30180509315977483\n",
      "[Epoch/Iterations]:[40/200], loss: 0.3377742740985289\n",
      "[Epoch/Iterations]:[40/300], loss: 0.38014117581806417\n",
      "[Epoch/Iterations]:[40/400], loss: 0.2405647167748845\n",
      "[Epoch/Iterations]:[40/500], loss: 0.16536851608701914\n",
      "=> Elapsed time epoch #40 : 0.50 seconds\n",
      "[Epoch/Iterations]:[41/0], loss: 0.23413439876113276\n",
      "[Epoch/Iterations]:[41/100], loss: 0.37781528927743824\n",
      "[Epoch/Iterations]:[41/200], loss: 0.1779620452208751\n",
      "[Epoch/Iterations]:[41/300], loss: 0.46572605130111744\n",
      "[Epoch/Iterations]:[41/400], loss: 0.23368734027447868\n",
      "[Epoch/Iterations]:[41/500], loss: 0.3928196063653139\n",
      "=> Elapsed time epoch #41 : 0.49 seconds\n",
      "[Epoch/Iterations]:[42/0], loss: 0.25666518373164404\n",
      "[Epoch/Iterations]:[42/100], loss: 0.23044538444969462\n",
      "[Epoch/Iterations]:[42/200], loss: 0.3217740410304007\n",
      "[Epoch/Iterations]:[42/300], loss: 0.3056761028868529\n",
      "[Epoch/Iterations]:[42/400], loss: 0.21307866945635737\n",
      "[Epoch/Iterations]:[42/500], loss: 0.2137522735441685\n",
      "=> Elapsed time epoch #42 : 0.49 seconds\n",
      "[Epoch/Iterations]:[43/0], loss: 0.27097469525072465\n",
      "[Epoch/Iterations]:[43/100], loss: 0.15422029725253597\n",
      "[Epoch/Iterations]:[43/200], loss: 0.1990909287701242\n",
      "[Epoch/Iterations]:[43/300], loss: 0.21990473000680674\n",
      "[Epoch/Iterations]:[43/400], loss: 0.31547057858452054\n",
      "[Epoch/Iterations]:[43/500], loss: 0.2903733463451736\n",
      "=> Elapsed time epoch #43 : 0.50 seconds\n",
      "[Epoch/Iterations]:[44/0], loss: 0.40715552709526276\n",
      "[Epoch/Iterations]:[44/100], loss: 0.15999315440555012\n",
      "[Epoch/Iterations]:[44/200], loss: 0.28728499606747737\n",
      "[Epoch/Iterations]:[44/300], loss: 0.36020888395645456\n",
      "[Epoch/Iterations]:[44/400], loss: 0.24940048038354698\n",
      "[Epoch/Iterations]:[44/500], loss: 0.3566884184087138\n",
      "=> Elapsed time epoch #44 : 0.50 seconds\n",
      "[Epoch/Iterations]:[45/0], loss: 0.1946724539092974\n",
      "[Epoch/Iterations]:[45/100], loss: 0.13198935503235903\n",
      "[Epoch/Iterations]:[45/200], loss: 0.21454093176520309\n",
      "[Epoch/Iterations]:[45/300], loss: 0.23692396092566723\n",
      "[Epoch/Iterations]:[45/400], loss: 0.24702847589028132\n",
      "[Epoch/Iterations]:[45/500], loss: 0.2557244271790008\n",
      "=> Elapsed time epoch #45 : 0.50 seconds\n",
      "[Epoch/Iterations]:[46/0], loss: 0.27096670952244306\n",
      "[Epoch/Iterations]:[46/100], loss: 0.17628176600250772\n",
      "[Epoch/Iterations]:[46/200], loss: 0.17719214812288003\n",
      "[Epoch/Iterations]:[46/300], loss: 0.14556038456244616\n",
      "[Epoch/Iterations]:[46/400], loss: 0.24028860104514932\n",
      "[Epoch/Iterations]:[46/500], loss: 0.3563402908023936\n",
      "=> Elapsed time epoch #46 : 0.50 seconds\n",
      "[Epoch/Iterations]:[47/0], loss: 0.18463212943474172\n",
      "[Epoch/Iterations]:[47/100], loss: 0.16705286562797\n",
      "[Epoch/Iterations]:[47/200], loss: 0.27407081119953397\n",
      "[Epoch/Iterations]:[47/300], loss: 0.30693634297838024\n",
      "[Epoch/Iterations]:[47/400], loss: 0.29984842466305045\n",
      "[Epoch/Iterations]:[47/500], loss: 0.2793831771140794\n",
      "=> Elapsed time epoch #47 : 0.50 seconds\n",
      "[Epoch/Iterations]:[48/0], loss: 0.22127756173182658\n",
      "[Epoch/Iterations]:[48/100], loss: 0.2736989008446821\n",
      "[Epoch/Iterations]:[48/200], loss: 0.2715686659937617\n",
      "[Epoch/Iterations]:[48/300], loss: 0.2537076501791533\n",
      "[Epoch/Iterations]:[48/400], loss: 0.1782791451925186\n",
      "[Epoch/Iterations]:[48/500], loss: 0.2264509658857252\n",
      "=> Elapsed time epoch #48 : 0.50 seconds\n",
      "[Epoch/Iterations]:[49/0], loss: 0.22005873206663754\n",
      "[Epoch/Iterations]:[49/100], loss: 0.23229609103813317\n",
      "[Epoch/Iterations]:[49/200], loss: 0.21385928622793274\n",
      "[Epoch/Iterations]:[49/300], loss: 0.21444660357307843\n",
      "[Epoch/Iterations]:[49/400], loss: 0.2913574066441082\n",
      "[Epoch/Iterations]:[49/500], loss: 0.19214199944742513\n",
      "=> Elapsed time epoch #49 : 0.50 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define an interactive ipython figure to display the training loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(\"Training loss\")\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.ion()\n",
    "#fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# Log the training loss\n",
    "loss_log = []\n",
    "# Zeros initialize the momentum for SGD\n",
    "V_dW1 = np.zeros(W1.shape)\n",
    "V_dW2 = np.zeros(W2.shape)\n",
    "\n",
    "L = 0\n",
    "\n",
    "# Training\n",
    "for i in range(epoch):\n",
    "    start_t = time.time()\n",
    "    \n",
    "    # Random shuffle training data every training epoch\n",
    "    np.random.seed(np.random.randint(num_trains))\n",
    "    indices = np.random.permutation(num_trains)\n",
    "    X_train_shuffled, Y_train_shuffled = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "    for j in range(num_batchs):\n",
    "\n",
    "        # Get mini-batch samples for training\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = min(j * batch_size + batch_size, X_train.shape[1] - 1)\n",
    "        X, Y = X_train_shuffled[:, start_idx : end_idx], Y_train_shuffled[:, start_idx : end_idx]\n",
    "        # Size of actual mini-batch, it could be smaller than batch_size\n",
    "        mini_batch = end_idx - start_idx\n",
    "        \n",
    "        \n",
    "        # TODO: implement the forward-pass (0.5 point)\n",
    "        W1, W2 = W1.reshape(num_hidden, num_input), W2.reshape(num_classes, num_hidden)\n",
    "        #print(np.shape(X))\n",
    "        #print(np.shape(W2))\n",
    "        Z1 = np.matmul(W1,X)\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.matmul(W2,A1)\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        if is_weight_decay: \n",
    "            # TODO: call cross entropy loss with weight decay regularization\n",
    "            # (0.2 point)\n",
    "            L = cross_entropy_loss(Y, A2) + (lmda / (2 * mini_batch)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "            \n",
    "        else:\n",
    "            # TODO: call cross entropy loss (0.5 point)\n",
    "            L = cross_entropy_loss(Y, A2)\n",
    "        #a = np.matmul(A2,Y)\n",
    "        # Log the training loss during training\n",
    "        loss_log.append(L)\n",
    "        #a = np.matmul(Y,Y)\n",
    "        \n",
    "        # Gradient check\n",
    "        if is_gradient_check:\n",
    "            numgrad = calculate_numerical_gradient(W1, W2, X, Y, is_weight_decay)\n",
    "\n",
    "        # TODO: calculate the derivative of 𝐿 with respect to 𝑍2 using eq. 8 (0.5 point)\n",
    "        #L_Z2 = (-Y/A2 + (1-Y)/(1-A2)) * A2 * (1 - A2)\n",
    "        L_Z2 = np.divide(A2 - Y, Y.shape[1])\n",
    "        \n",
    "        if is_weight_decay:\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 6 with weight decay regularization\n",
    "            # (0.2 point)\n",
    "            L_W2 = np.matmul(L_Z2,np.transpose(A1)) + (lmda / (2 * mini_batch)) * W2\n",
    "            \n",
    "        else:\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 6 (0.5 point)\n",
    "            L_W2 = np.matmul(L_Z2,np.transpose(A1))\n",
    "            #L_W2 = np.matmul(L_Z2,np.transpose(A1))\n",
    "        \n",
    "        # TODO: calculate the derivative of 𝐿 with respect to A1 using eq. 10 (0.5 point)\n",
    "        #L_A1 = np.matmul(np.transpose(W2), L_Z2)\n",
    "        #print(np.shape(W2))\n",
    "        #print(np.shape(A2))\n",
    "        L_A1 = np.matmul(np.transpose(W2),L_Z2)\n",
    "        \n",
    "        \n",
    "        # TODO: calculate the derivative of 𝐿 with respect to Z1 using eq. 10 and eq. 11 (0.5 point)\n",
    "        #L_Z1 = L_A1 * A1 * (1 - A1)\n",
    "        L_Z1 = L_A1 * A1 * (1 - A1)\n",
    "        \n",
    "        if is_weight_decay:\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 7 with weight decay regularization\n",
    "            # (0.2 point)\n",
    "            L_W1 = np.matmul(L_Z1,np.transpose(X)) + (lmda / (2 * mini_batch)) * W1\n",
    "                                                       \n",
    "        else:\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 7 (0.5 point)\n",
    "            L_W1 = np.matmul(L_Z1, np.transpose(X))\n",
    "        \n",
    "        dW1 = L_W1\n",
    "        dW2 = L_W2\n",
    "\n",
    "        # Gradient check\n",
    "        if is_gradient_check:\n",
    "            # Flatten and combine grad vector\n",
    "            grad = np.concatenate((dW1.ravel(), dW2.ravel()))\n",
    "            #print(grad)\n",
    "            #print(numgrad)\n",
    "            # Compare num_grad and grad\n",
    "            grad_diff = np.linalg.norm(grad - numgrad) / np.linalg.norm(grad + numgrad)\n",
    "            #print(\"Grad_diff: {}\".format(grad_diff))\n",
    "            assert grad_diff < 1e-8, \"Gradient check fail, please carefully inspect your gradient calculations again!\"        \n",
    "        # TODO: Update the learning velocity using Eq. 17 (0.5 point)\n",
    "        \n",
    "        V_dW1 = beta * V_dW1 + (1 - beta) * dW1\n",
    "        V_dW2 = beta * V_dW2 + (1 - beta) * dW2\n",
    "\n",
    "        # TODO: Update the model weights using Eq. 18 (0.5 point)\n",
    "\n",
    "        W1 = W1 - learning_rate * V_dW1\n",
    "        W2 = W2 - learning_rate * V_dW2\n",
    "\n",
    "        if (j % 100 == 0):\n",
    "            print(\"[Epoch/Iterations]:[{}/{}], loss: {}\".format(i, j, L))\n",
    "            \n",
    "    ax.clear()\n",
    "    ax.plot(loss_log)\n",
    "    fig.canvas.draw()\n",
    "    print(\"=> Elapsed time epoch #{} : {:.2f} seconds\".format(i, time.time() - start_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of your model (<span style=\"color:green\">0.5 points</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[865   3  22  26   4   0  70   0  10   0]\n",
      " [  5 962   4  22   4   0   2   0   1   0]\n",
      " [ 17   2 851  16  78   0  34   0   2   0]\n",
      " [ 30   8  13 905  24   0  18   0   2   0]\n",
      " [  0   1 117  42 801   0  34   0   5   0]\n",
      " [  0   0   0   1   0 943   0  40   1  15]\n",
      " [151   0 148  28  93   1 561   0  18   0]\n",
      " [  0   0   0   0   0  16   0 970   0  14]\n",
      " [  2   0   8   7   2   1   3   6 971   0]\n",
      " [  0   0   0   0   0   8   1  57   0 934]]\n",
      "Testing accuracy: 0.8763\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement the forward-pass (0.5 point)\n",
    "\n",
    "Z1 = np.matmul(W1,X_test)\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2,A1)\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# Evaluate the performance of your NN\n",
    "predictions = np.argmax(A2, axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(labels, predictions)))\n",
    "print(\"Testing accuracy: {}\".format(accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Regularization and NN simple tunning (<span style=\"color:green\">2 points</span>)\n",
    "\n",
    "1. Applying weight decay (<span style=\"color:green\">1 point</span>)\n",
    "  * Using what you learnt from assignment 1 to add the code at neccesary parts in **Training your network** and **Implement gradient check** above.\n",
    "  * There are 5 spots, (<span style=\"color:green\">0.2 point</span>) each spot.\n",
    "2. Change the number of neurons in the hidden layer and report the performance (<span style=\"color:green\">1 point</span>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer of question 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the carried experiments, it looks like the best results is obtained using 55 hidden neurons, with the below configuration. As a rule of thumb, the more hidden neurons, the better. Of course that there is a saturation point, that can be found by using an additional validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag use to enable/disable gradient check and weight decay regularization\n",
    "is_gradient_check = False\n",
    "is_weight_decay = True\n",
    "\n",
    "if is_weight_decay:\n",
    "    # Setting lambda coefficient for weight decay\n",
    "    lmda = np.exp(-7)\n",
    "\n",
    "# Seting learning rate and momentum for SGD\n",
    "learning_rate = 0.25\n",
    "beta = 0.4\n",
    "\n",
    "# Seting the number of training epochs\n",
    "epoch = 50\n",
    "# Choose your batch size\n",
    "batch_size = 100\n",
    "# Calculate the number of training iterations base on the number of training samples and your batch size\n",
    "num_batchs = num_trains // batch_size\n",
    "\n",
    "print(\"Num_trains: {}, num_batchs: {}\".format(num_trains, num_batchs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for 5 hidden neurons: 0.7759\n",
      "Testing accuracy for 15 hidden neurons: 0.8557\n",
      "Testing accuracy for 25 hidden neurons: 0.8678\n",
      "Testing accuracy for 35 hidden neurons: 0.8734\n",
      "Testing accuracy for 45 hidden neurons: 0.876\n",
      "Testing accuracy for 55 hidden neurons: 0.8778\n",
      "Testing accuracy for 65 hidden neurons: 0.8732\n"
     ]
    }
   ],
   "source": [
    "for num_hidden in range(5,75,10):\n",
    "    W1 = np.random.randn(num_hidden,num_input) * np.sqrt(2 / num_hidden)\n",
    "    W2 = np.random.randn(num_classes,num_hidden) * np.sqrt(2 / num_hidden)\n",
    "    \n",
    "    loss_log = []\n",
    "    V_dW1 = np.zeros(W1.shape)\n",
    "    V_dW2 = np.zeros(W2.shape)\n",
    "    L = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        start_t = time.time()\n",
    "\n",
    "        # Random shuffle training data every training epoch\n",
    "        np.random.seed(np.random.randint(num_trains))\n",
    "        indices = np.random.permutation(num_trains)\n",
    "        X_train_shuffled, Y_train_shuffled = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "        for j in range(num_batchs):\n",
    "\n",
    "            # Get mini-batch samples for training\n",
    "            start_idx = j * batch_size\n",
    "            end_idx = min(j * batch_size + batch_size, X_train.shape[1] - 1)\n",
    "            X, Y = X_train_shuffled[:, start_idx : end_idx], Y_train_shuffled[:, start_idx : end_idx]\n",
    "            # Size of actual mini-batch, it could be smaller than batch_size\n",
    "            mini_batch = end_idx - start_idx\n",
    "\n",
    "\n",
    "            # TODO: implement the forward-pass (0.5 point)\n",
    "            W1, W2 = W1.reshape(num_hidden, num_input), W2.reshape(num_classes, num_hidden)\n",
    "            #print(np.shape(X))\n",
    "            #print(np.shape(W2))\n",
    "            Z1 = np.matmul(W1,X)\n",
    "            A1 = sigmoid(Z1)\n",
    "            Z2 = np.matmul(W2,A1)\n",
    "            A2 = softmax(Z2)\n",
    "\n",
    "            if is_weight_decay: \n",
    "                # TODO: call cross entropy loss with weight decay regularization\n",
    "                # (0.2 point)\n",
    "                L = cross_entropy_loss(Y, A2) + (lmda / (2 * mini_batch)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "\n",
    "            else:\n",
    "                # TODO: call cross entropy loss (0.5 point)\n",
    "                L = cross_entropy_loss(Y, A2)\n",
    "            #a = np.matmul(A2,Y)\n",
    "            # Log the training loss during training\n",
    "            loss_log.append(L)\n",
    "            #a = np.matmul(Y,Y)\n",
    "\n",
    "            # Gradient check\n",
    "            if is_gradient_check:\n",
    "                numgrad = calculate_numerical_gradient(W1, W2, X, Y, is_weight_decay)\n",
    "\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to 𝑍2 using eq. 8 (0.5 point)\n",
    "            #L_Z2 = (-Y/A2 + (1-Y)/(1-A2)) * A2 * (1 - A2)\n",
    "            L_Z2 = np.divide(A2 - Y, Y.shape[1])\n",
    "\n",
    "            if is_weight_decay:\n",
    "                # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 6 with weight decay regularization\n",
    "                # (0.2 point)\n",
    "                L_W2 = np.matmul(L_Z2,np.transpose(A1)) + (lmda / (2 * mini_batch)) * W2\n",
    "\n",
    "            else:\n",
    "                # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 6 (0.5 point)\n",
    "                L_W2 = np.matmul(L_Z2,np.transpose(A1))\n",
    "                #L_W2 = np.matmul(L_Z2,np.transpose(A1))\n",
    "\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to A1 using eq. 10 (0.5 point)\n",
    "            #L_A1 = np.matmul(np.transpose(W2), L_Z2)\n",
    "            #print(np.shape(W2))\n",
    "            #print(np.shape(A2))\n",
    "            L_A1 = np.matmul(np.transpose(W2),L_Z2)\n",
    "\n",
    "\n",
    "            # TODO: calculate the derivative of 𝐿 with respect to Z1 using eq. 10 and eq. 11 (0.5 point)\n",
    "            #L_Z1 = L_A1 * A1 * (1 - A1)\n",
    "            L_Z1 = L_A1 * A1 * (1 - A1)\n",
    "\n",
    "            if is_weight_decay:\n",
    "                # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 7 with weight decay regularization\n",
    "                # (0.2 point)\n",
    "                L_W1 = np.matmul(L_Z1,np.transpose(X)) + (lmda / (2 * mini_batch)) * W1\n",
    "\n",
    "            else:\n",
    "                # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 7 (0.5 point)\n",
    "                L_W1 = np.matmul(L_Z1, np.transpose(X))\n",
    "\n",
    "            dW1 = L_W1\n",
    "            dW2 = L_W2\n",
    "\n",
    "            # Gradient check\n",
    "            if is_gradient_check:\n",
    "                # Flatten and combine grad vector\n",
    "                grad = np.concatenate((dW1.ravel(), dW2.ravel()))\n",
    "                #print(grad)\n",
    "                #print(numgrad)\n",
    "                # Compare num_grad and grad\n",
    "                grad_diff = np.linalg.norm(grad - numgrad) / np.linalg.norm(grad + numgrad)\n",
    "                #print(\"Grad_diff: {}\".format(grad_diff))\n",
    "                assert grad_diff < 1e-8, \"Gradient check fail, please carefully inspect your gradient calculations again!\"        \n",
    "            # TODO: Update the learning velocity using Eq. 17 (0.5 point)\n",
    "\n",
    "            V_dW1 = beta * V_dW1 + (1 - beta) * dW1\n",
    "            V_dW2 = beta * V_dW2 + (1 - beta) * dW2\n",
    "\n",
    "            # TODO: Update the model weights using Eq. 18 (0.5 point)\n",
    "\n",
    "            W1 = W1 - learning_rate * V_dW1\n",
    "            W2 = W2 - learning_rate * V_dW2\n",
    "        \n",
    "\n",
    "    Z1 = np.matmul(W1,X_test)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(W2,A1)\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # Evaluate the performance of your NN\n",
    "    predictions = np.argmax(A2, axis=0)\n",
    "    labels = np.argmax(Y_test, axis=0)\n",
    "    print(\"Testing accuracy for {} hidden neurons: {}\".format(num_hidden, accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
